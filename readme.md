##### Table of contents
1. [Environment setup](#environment-setup)
2. [Dataset preparation](#dataset-preparation)
3. [Training](#training)
4. [Evaluation](#evaluation)
5. [Contacts](#contacts)

# Official PyTorch implementation of "A High-Quality Robust Diffusion Framework for Corrupted Dataset" (ECCV'24)

<div align="center">
  <a href="https://quandao10.github.io/" target="_blank">Quan&nbsp;Dao</a> &emsp;
  <a href="https://github.com/Tahuubinh" target="_blank">Binh&nbsp;Ta</a> &emsp;
  <a href="https://github.com/" target="_blank">Tung&nbsp;Pham</a> &emsp;
  <a href="https://sites.google.com/site/anhttranusc/" target="_blank">Anh&nbsp;Tran</a>
  <br> <br>
  
  
  <a href="https://www.vinai.io/">VinAI Research</a>
</div>

> **Abstract**: Developing image-generative models, which are robust to outliers in the training process, has recently drawn attention from the research community. Due to the ease of integrating unbalanced optimal transport (UOT) into adversarial framework, existing works focus mainly on developing robust frameworks for generative adversarial model (GAN). Meanwhile, diffusion models have recently dominated GAN in various tasks and datasets. However, according to our knowledge, none of them are robust to corrupted datasets. Motivated by DDGAN, our work introduces the first robust-to-outlier diffusion. We suggest replacing the UOT-based generative model for GAN in DDGAN to learn the backward diffusion process. Additionally, we demonstrate that the Lipschitz property of divergence in our framework contributes to more stable training convergence. Remarkably, our method not only exhibits robustness to corrupted datasets but also achieves superior performance on clean datasets.

**TLDR**: This work introduces the first robust-to-outlier diffusion and suggests replacing the UOT-based generative model for GAN in DDGAN to learn the backward diffusion process, and demonstrates that the Lipschitz property of divergence in the framework contributes to more stable training convergence.

Details of algorithms, experimental results and configurations can be found in [our following paper](https://www.semanticscholar.org/paper/A-High-Quality-Robust-Diffusion-Framework-for-Dao-Ta/146988925950eb7ffdb0b854799946cc4b4a7fc8):
```bibtex
@inproceedings{Dao2023AHR,
  title={A High-Quality Robust Diffusion Framework for Corrupted Dataset},
  author={Quan Dao and Binh Ta and Tung Pham and Anh Tran},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:265498823}
}
```
**Please CITE** our paper whenever this repository is used to help produce published results or incorporated into other software.

## Environment setup
First, install Pytorch v1.12.1:
```
pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu116
```
Then, install other modules using:
```
pip install -r requirements.txt
```
Pretrained checkpoints can be **downloaded** from this [link](https://drive.google.com/drive/folders/1e7FyELPlqnoHJPpehvDv9Mi6nta-78n4?usp=sharing)

## Set up datasets ##
We trained on several datasets, including CIFAR10, LSUN Church Outdoor 256, CelebA HQ 256, MNIST. 



## Training ##
We use the following commands for training our proposed model.

#### CIFAR-10 pertubed by MNIST (5%) ####

We train RDUOT using 1 32-GB V100 GPU. 
```
python3 train.py --dataset cifar10 --num_channels 3 --num_channels_dae 128 --num_timesteps 4 --num_res_blocks 2 --batch_size 256 --num_epoch 1800 --ngf 64 --nz 100 --z_emb_dim 256 --n_mlp 4 --embedding_type positional --use_ema --ema_decay 0.9999 --r1_gamma 0.02 --lr_d 1.25e-4 --lr_g 1.6e-4 --lazy_reg 15 --num_process_per_node 1 --ch_mult 1 2 2 2 --save_content --version bs256 --master_port 6021 --phi1 softplus --phi2 softplus --perturb_dataset mnist --perturb_percent 5
```

Note: Remove `--perturb_dataset` and `--perturb_percent` for a clean training dataset.

For the configurations of other experiments, please refer to the paper.

## Evaluation ##
After training, samples can be generated by calling ```test.py```. 
Below, we use -`-epoch_start` (first epoch), `--epoch_end` (last epoch), `--epoch_jump` (number of epochs before the next evaluation) to specify the checkpoint saved at a particular epoch.
Specifically, for models trained by above commands, the scripts for generating samples on CIFAR-10 (DDGAN) is
```
python3 test.py --dataset cifar10 --num_channels 3 --num_channels_dae 128 --num_timesteps 4 --batch_size 1800 --num_res_blocks 2 --nz 100 --z_emb_dim 256 --n_mlp 4 --ch_mult 1 2 2 2 --version bs256 --master_port 6038 --compute_fid --epoch_start 1100 --epoch_end 1800 --epoch_jump 25 --phi1 softplus --phi2 softplus --perturb_dataset mnist --perturb_percent 5
```

We use the [PyTorch](https://github.com/mseitzer/pytorch-fid) implementation to compute the FID scores, and in particular, codes for computing the FID are adapted from [FastDPM](https://github.com/FengNiMa/FastDPM_pytorch).

To compute FID, run the same scripts above for sampling, with additional arguments ```--compute_fid```.

Code for computing Inception Score is adapted from [here](https://github.com/tsc2017/Inception-Score).

For Improved Precision and Recall, follow the instruction [here](https://github.com/kynkaat/improved-precision-and-recall-metric).

## Contacts
If you have any problems, please open an issue in this repository or send an email to [kevinquandao10@gmail.com](mailto:kevinquandao10@gmail.com) or [tahuubinh2001@gmail.com](tahuubinh2001@gmail.com).

## License
```
Copyright (c) 2024 VinAI
Licensed under the Creative Commons Attribution Non Commercial 4.0 International.
You may obtain a copy of the License at
    https://creativecommons.org/licenses/by-nc/4.0/
```
